{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " NeuralNetBackpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtyospowtbbkSul5GQCobm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liberty0081/DeepLearningColab/blob/main/NeuralNetBackpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQs608TXNIf5",
        "outputId": "46abbc6a-7fb8-4f7c-b683-6fe05c906826"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/NeuralNetwork')\n",
        "\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import keras\n",
        "from NeuralNetworkOptimizer import *\n",
        "from NeuralNetworkLayer import *\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self):\n",
        "        #ネットワークの情報のリスト\n",
        "        self.data                     = []\n",
        "        #ネットワークの実体のリスト\n",
        "        self.layer                    = []\n",
        "        #実装済みの活性化関数のリスト\n",
        "        self.activation_function_list = [\"Swish\", \"ReLU\", \"Sigmoid\"]\n",
        "        #実装済みの出力層のリスト\n",
        "        self.output_layer_list        = [\"Softmax2CrossEntropy\"]\n",
        "        #実装済みの最適化手法のリスト\n",
        "        self.optimizer_list           = [\"SDG\", \"Adam\", \"Momentum\", \"AdaGrad\"]\n",
        "\n",
        "    #ニューラルネットに活性化関数を加えるメソッド\n",
        "    def add_activation_function(self, func, size, indx = None):\n",
        "        pass\n",
        "\n",
        "    #ニューラルネットに全結合層(Affine層)を加えるメソッド\n",
        "    def add_affine(self, input, output, optimizer, lr = 0.01, arg1 = None, arg2 = None, init = None, indx = None):\n",
        "        pass\n",
        "\n",
        "    #ニューラルネットにBatch Norm層を加えるメソッド\n",
        "    def add_batch_norm(self, size, optimizer, lr = 0.01, arg1 = None, arg2 = None, indx = None):\n",
        "        pass\n",
        "\n",
        "    #ニューラルネットにOutput層を加えるメソッド\n",
        "    def add_output_layer(self, func, size, indx = None)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "class NeuralNetwork():\n",
        "    def __init__(self):\n",
        "        self.data                = {}\n",
        "        self.layer               = []\n",
        "        self.flags               = {\"IN\":False, \"OUT\":False, \"LAYERNUM\":False, \"NEURONNUM\":False, \"ACTFUNC\":False, \"OUTLAYER\":False, \"OPTIMIZER\":False}\n",
        "        self.actFuncList         = [\"Swish\", \"ReLU\", \"Sigmoid\"]\n",
        "        self.outLayerList        = [\"Softmax2CrossEntropy\"]\n",
        "        self.optimizerList       = [\"SDG\", \"Adam\", \"Momentum\", \"AdaGrad\"]\n",
        "        self.createFlag          = False\n",
        "        self.data[\"xavier\"]      = False\n",
        "        self.data[\"he\"]          = False\n",
        "        self.data[\"beforeBatch\"] = False\n",
        "        self.data[\"afterBatch\"]  = False\n",
        "        self.data[\"xavier\"]      = False\n",
        "        self.data[\"he\"]          = False\n",
        "\n",
        "    def __repr__(self):\n",
        "        #ここにprint関数で表示させたい情報(ニューラルネットワークの実態の情報)を表示させるようにする。\n",
        "        return \"ここにprint関数で表示させたい情報(ニューラルネットワークの実態の情報)を表示させるようにする。\"\n",
        "\n",
        "    def inputSize(self, num):\n",
        "        self.createFlag = False\n",
        "\n",
        "        if type(num) == int and num > 0:\n",
        "            self.data[\"input\"] = num\n",
        "            self.flags[\"IN\"]   = True\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"arg has to be <class 'int'> and 1 or more.\")\n",
        "\n",
        "    def outputSize(self, num):\n",
        "        self.createFlag = False\n",
        "\n",
        "        if type(num) == int and num > 0:\n",
        "            self.data[\"output\"] = num\n",
        "            self.flags[\"OUT\"]   = True\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"arg has to be <class 'int'> and 1 or more.\")\n",
        "\n",
        "    def layerNumber(self, num):\n",
        "        self.createFlag = False\n",
        "\n",
        "        if type(num) == int and num > 1:\n",
        "            self.data[\"layNum\"]    = num\n",
        "            self.flags[\"LAYERNUm\"] = True\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"arg has to be <class 'int'> and 2 or more.\")\n",
        "\n",
        "    def neuronNumber(self, x) :\n",
        "        self.createFlag = False\n",
        "\n",
        "        if self.data[\"layNum\"] is None:\n",
        "            raise Exception(\"you have to run layerNumber method before.\")\n",
        "\n",
        "        elif type(x) == int or type(x) == list:\n",
        "            if type(x) == int:\n",
        "                if x > 1:\n",
        "                    self.data[\"neuNum\"]     = [x]*(self.data[\"layNum\"] - 1)\n",
        "                    self.flags[\"NEURONNUM\"] = True\n",
        "                    \n",
        "                else:\n",
        "                    raise Exception(\"arg has to be 2 or more.\")\n",
        "\n",
        "            elif type(x) == list:\n",
        "                if len(x) == len([i for i in x if type(i) == int and i > 1]):\n",
        "                    if len(x) + 1 == self.data[\"layNum\"]:\n",
        "                        self.data[\"neuNum\"]     = x\n",
        "                        self.flags[\"NEURONNUM\"] = True\n",
        "                    \n",
        "                    else:\n",
        "                        raise Exception(\"length of list has to be 'arg of layerNumber - 1'.\")\n",
        "\n",
        "                else:\n",
        "                    raise Exception(\"elms of list have to be <class 'int'> and 2 or more\")\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"arg has to be <class 'int'> or <class 'list'>\")\n",
        "\n",
        "    def actFunction(self, name):\n",
        "        self.createFlag = False\n",
        "\n",
        "        if name in self.actFuncList:\n",
        "            self.data[\"actFunc\"]  = name\n",
        "            self.flags[\"ACTFUNC\"] = True\n",
        "        \n",
        "        else:\n",
        "            raise Exception(\"arg has to be an appropriate function name. appropriate function names are in NeuralNetwork.actFuncList.\")\n",
        "\n",
        "    def outputLayer(self, name):\n",
        "        self.createFlag = False\n",
        "\n",
        "        if name in self.outLayerList:\n",
        "            self.data[\"outLayer\"]  = name\n",
        "            self.flags[\"OUTLAYER\"] = True\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"arg has to be a layer name. appropriate layer names are in NeuralNetwork.outLayerList.\")\n",
        "\n",
        "    def batchNorm(self, afterAF = False):\n",
        "        self.createFlag = False\n",
        "\n",
        "        if not afterAF:\n",
        "            if self.data[\"beforeBatch\"]:\n",
        "                self.data[\"beforeBatch\"] = False\n",
        "\n",
        "            elif self.data[\"afterBatch\"]:\n",
        "                self.data[\"afterBatch\"] = False\n",
        "                self.data[\"beforeBatch\"] = True\n",
        "\n",
        "            else:\n",
        "                self.data[\"beforeBatch\"] = True\n",
        "\n",
        "        elif afterAF:\n",
        "            if self.data[\"afterBatch\"]:\n",
        "                self.data[\"afterBatch\"] = False\n",
        "\n",
        "            elif self.data[\"beforeBatch\"]:\n",
        "                self.data[\"beforeBatch\"] = False\n",
        "                self.data[\"afterBatch\"] = True\n",
        "\n",
        "            else:\n",
        "                self.data[\"afterBatch\"] = True\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"arg has to be <class 'bool'>.\")\n",
        "\n",
        "    def xavier(self):\n",
        "        self.createFlag = False\n",
        "        self.data[\"he\"] = False\n",
        "\n",
        "        if self.data[\"xavier\"]:\n",
        "            self.data[\"xavier\"] = False\n",
        "\n",
        "        else:\n",
        "            self.data[\"xavier\"] = True\n",
        "\n",
        "    def he(self):\n",
        "        self.createFlag     = False\n",
        "        self.data[\"xavier\"] = False\n",
        "\n",
        "        if self.data[\"he\"]:\n",
        "            self.data[\"he\"] = False\n",
        "\n",
        "        else:\n",
        "            self.data[\"he\"] = True\n",
        "\n",
        "    def optimizer(self, name, lr = None, arg1 = None, arg2 = None):\n",
        "        self.createFlag = False\n",
        "\n",
        "        if name in self.optimizerList:\n",
        "            if (type(lr) == float or lr is None) and (type(arg1) == float or arg1 is None) and (type(arg2) == float or arg2 is None):\n",
        "                self.data[\"optimizer\"][\"name\"] = name\n",
        "                self.data[\"optimizer\"][\"lr\"]   = lr\n",
        "                self.data[\"optimizer\"][\"arg1\"] = arg1\n",
        "                self.data[\"optimizer\"][\"arg2\"] = arg2\n",
        "                self.flags[\"OPTIMIZER\"]        = True\n",
        "\n",
        "            else:\n",
        "                raise Exception(\"args have to be <class 'float'> or <class 'NoneType'>\")\n",
        "        \n",
        "        else:\n",
        "            raise Exception(\"name has to be a optimizer name. appropriate optimizer names are in NeuralNetwork.optimizerList.\")\n",
        "\n",
        "    def create(self):\n",
        "        if not False in self.flags.values():\n",
        "            num_list = self.data[\"neuNum\"]\n",
        "            num_list.insert(0, self.data[\"input\"])\n",
        "            num_list.append(self.datta[\"output\"])\n",
        "\n",
        "            for i in range(self.data[\"layNum\"] - 1):\n",
        "                dict = {}\n",
        "                dict[\"Affine\"] = Affine(num_list[i], num_list[i + 1])\n",
        "\n",
        "                if self.data[\"actFunc\"] == \"Swish\":\n",
        "                    dict[\"ActivationFunction\"] = Swish()\n",
        "                \n",
        "                if self.data[\"actFunc\"] == \"ReLU\":\n",
        "                    dict[\"ActivationFunction\"] = ReLU()\n",
        "\n",
        "                if self.data[\"actFunc\"] == \"Sigmoid\":\n",
        "                    dict[\"ActivationFunction\"] = Sigmoid()\n",
        "\n",
        "                if self.data[\"beforeBatch\"]:\n",
        "                    dict[\"BeforeBatchNorm\"] = BatchNorm()\n",
        "\n",
        "                if self.data[\"afterBatch\"]:\n",
        "                    dict[\"AfterBatchNorm\"] = BatchNorm()\n",
        "\n",
        "                if self.data[\"optimizer\"][\"name\"] == \"SDG\":\n",
        "                    if self.data[\"optimizer\"][\"lr\"] is None:\n",
        "                        dict[\"Optimizer\"] = SDG()\n",
        "\n",
        "                    else:\n",
        "                        dict[\"Optimizer\"] = SDG(lr = self.data[\"optimizer\"][\"lr\"])\n",
        "\n",
        "                if self.data[\"optimizer\"][\"name\"] == \"Momentum\":\n",
        "                    if self.data[\"optimizer\"][\"lr\"] is None:\n",
        "                        dict[\"Optimizer\"] = Momentum(rv = self.data[\"optimizer\"][\"arg1\"])\n",
        "\n",
        "                    else:\n",
        "                        dict[\"Optimizer\"] = Momentum(lr = self.data[\"optimizer\"][\"lr\"], rv = self.data[\"optimizer\"][\"arg1\"])\n",
        "\n",
        "                if self.data[\"optimizer\"][\"name\"] == \"AdaGrad\":\n",
        "                    if self.data[\"optimizer\"][\"lr\"] is None:\n",
        "                        dict[\"Optimizer\"] = AdaGrad()\n",
        "\n",
        "                    else:\n",
        "                        dict[\"Optimizer\"] = AdaGrad(lr = self.data[\"optimizer\"][\"lr\"])\n",
        "\n",
        "                if self.data[\"optimizer\"][\"name\"] == \"Adam\":\n",
        "                    if self.data[\"optimizer\"][\"lr\"] is None :\n",
        "                        dict[\"Optimizer\"] = Adam(beta1 = self.data[\"optimizer\"][\"arg1\"], beta2 = self.data[\"optimizer\"][\"arg2\"])\n",
        "\n",
        "                    else:\n",
        "                        dict[\"Optimizer\"] = Adam(lr = self.data[\"optimizer\"][\"lr\"], beta1 = self.data[\"optimizer\"][\"arg1\"], beta2 = self.data[\"optimizer\"][\"arg2\"])\n",
        "\n",
        "                self.layer.append(dict)\n",
        "\n",
        "            dict = {}\n",
        "            dict[\"Affine\"] = Affine(num_list[self.data[\"layNum\"] - 1], num_list[self.data[\"layNum\"]])\n",
        "\n",
        "            if self.data[\"outLayer\"] == \"Softmax2CrossEntropy\":\n",
        "                dict[\"OutputFunction\"] = Softmax2CrossEntropy()\n",
        "\n",
        "            if self.data[\"optimizer\"][\"name\"] == \"SDG\":\n",
        "                if self.data[\"optimizer\"][\"lr\"] is None:\n",
        "                    dict[\"Optimizer\"] = SDG()\n",
        "\n",
        "                else:\n",
        "                    dict[\"Optimizer\"] = SDG(lr = self.data[\"optimizer\"][\"lr\"])\n",
        "\n",
        "            if self.data[\"optimizer\"][\"name\"] == \"Momentum\":\n",
        "                if self.data[\"optimizer\"][\"lr\"] is None:\n",
        "                    dict[\"Optimizer\"] = Momentum(rv = self.data[\"optimizer\"][\"arg1\"])\n",
        "\n",
        "                else:\n",
        "                    dict[\"Optimizer\"] = Momentum(lr = self.data[\"optimizer\"][\"lr\"], rv = self.data[\"optimizer\"][\"arg1\"])\n",
        "\n",
        "            if self.data[\"optimizer\"][\"name\"] == \"AdaGrad\":\n",
        "                if self.data[\"optimizer\"][\"lr\"] is None:\n",
        "                    dict[\"Optimizer\"] = AdaGrad()\n",
        "\n",
        "                else:\n",
        "                    dict[\"Optimizer\"] = AdaGrad(lr = self.data[\"optimizer\"][\"lr\"])\n",
        "\n",
        "            if self.data[\"optimizer\"][\"name\"] == \"Adam\":\n",
        "                if self.data[\"optimizer\"][\"lr\"] is None :\n",
        "                    dict[\"Optimizer\"] = Adam(beta1 = self.data[\"optimizer\"][\"arg1\"], beta2 = self.data[\"optimizer\"][\"arg2\"])\n",
        "\n",
        "                else:\n",
        "                    dict[\"Optimizer\"] = Adam(lr = self.data[\"optimizer\"][\"lr\"], beta1 = self.data[\"optimizer\"][\"arg1\"], beta2 = self.data[\"optimizer\"][\"arg2\"])\n",
        "\n",
        "            self.layer.append(dict)\n",
        "\n",
        "            if self.data[\"xavier\"]:\n",
        "                for j in range(self.data[\"layNum\"]):\n",
        "                    self.layer[j][\"Affine\"].W = np.random.randn(self.layer[j][\"Affine\"].data[\"IN\"], self.layer[j][\"Affine\"].data[\"OUT\"])/np.aqrt(self.layer[j][\"Affine\"].data[\"IN\"])\n",
        "\n",
        "            if self.data[\"he\"]:\n",
        "                for j in range(self.data[\"layNum\"]):\n",
        "                    self.layer[j][\"Affine\"].W = np.random.randn(self.layer[j][\"Affine\"].data[\"IN\"], self.layer[j][\"Affine\"].data[\"OUT\"])*np.aqrt(2/self.layer[j][\"Affine\"].data[\"IN\"])\n",
        "\n",
        "            self.createFlag = True\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"you have to run appropriate method.\")\n",
        "                    \n",
        "class NeuralNetwork():\n",
        "    def __init__(self, inputSize, outputSize, layerNumber, neuronNumber, inputLayer = \"Sigmoid\", outputLayer = \"Softmax2CrossEntropy\", \\\n",
        "                 optimizer = \"SDG\", learningRate = 0.01, momentumRV = None, adamBeta1 = None, adamBeta2 = None):\n",
        "        self.data        = {\"IN\":inputSize, \"OUT\":outputSize, \"LayerNumber\":layerNumber, \"NeuronNumber\":neuronNumber, \\\n",
        "                            \"LearningRate\":learningRate, \"MomentumRV\":momentumRV, \"AdamBeta1\":adamBeta1, \"AdamBeta2\":adamBeta2}\n",
        "        self.layer       = []\n",
        "        self.optimizer   = []\n",
        "\n",
        "        if inputLayer == \"Swish\":\n",
        "            self.layer.append((Affine(self.data[\"IN\"], self.data[\"NeuronNumber\"]), Swish()))\n",
        "            for i in range(1, self.data[\"LayerNumber\"] - 1):\n",
        "                self.layer.append((Affine(self.data[\"NeuronNumber\"], self.data[\"NeuronNumber\"]), Swish()))\n",
        "\n",
        "        if inputLayer == \"Sigmoid\":\n",
        "            self.layer.append((Affine(inputSize=self.data[\"IN\"], outputSize=self.data[\"NeuronNumber\"]), Sigmoid()))\n",
        "            for i in range(1, self.data[\"LayerNumber\"] - 1):\n",
        "                self.layer.append((Affine(self.data[\"NeuronNumber\"], self.data[\"NeuronNumber\"]), Sigmoid()))\n",
        "\n",
        "        if inputLayer == \"ReLU\":\n",
        "            self.layer.append((Affine(self.data[\"IN\"], self.data[\"NeuronNumber\"]), ReLU()))\n",
        "            for i in range(1, self.data[\"LayerNumber\"] - 1):\n",
        "                self.layer.append(((Affine(self.data[\"NeuronNumber\"], self.data[\"NeuronNumber\"]), ReLU())))\n",
        "\n",
        "        if outputLayer == \"Softmax2CrossEntropy\":\n",
        "            self.layer.append((Affine(self.data[\"NeuronNumber\"], self.data[\"OUT\"]), Softmax2CrossEntropy()))\n",
        "\n",
        "        self.layer = tuple(self.layer)\n",
        "\n",
        "        if optimizer == \"SDG\":\n",
        "            for i in range(self.data[\"LayerNumber\"]):\n",
        "                self.optimizer.append(SDG(self.data[\"LearningRate\"]))\n",
        "\n",
        "        if optimizer == \"Momentum\":\n",
        "            for i in range(self.data[\"LayerNumber\"]):\n",
        "                self.optimizer.append(Momentum(self.data[\"LearningRate\"], self.data[\"MomentumRV\"]))\n",
        "\n",
        "        if optimizer == \"AdaGrad\":\n",
        "            for i in range(self.data[\"LayerNumber\"]):\n",
        "                self.optimizer.append(AdaGrad(self.data[\"LearningRate\"]))\n",
        "\n",
        "        if optimizer == \"Adam\":\n",
        "            for i in range(self.data[\"LayerNumber\"]):\n",
        "                self.optimizer.append(Adam(self.data[\"LearningRate\"], self.data[\"AdamBeta1\"], self.data[\"AdamBeta2\"]))\n",
        "\n",
        "\n",
        "        self.optimizer = tuple(self.optimizer)\n",
        "\n",
        "    def xavier(self):\n",
        "        for i in range(self.data[\"LayerNumber\"]):\n",
        "            self.layer[i][0].W = np.random.randn(self.layer[i][0].data[\"IN\"], self.layer[i][0].data[\"OUT\"])/np.sqrt(self.layer[i][0].data[\"IN\"])\n",
        "\n",
        "    def he(self):\n",
        "        for i in range(self.data[\"LayerNumber\"]):\n",
        "            self.layer[i][0].W = np.random.randn(self.layer[i][0].data[\"IN\"], self.layer[i][0].data[\"OUT\"])*np.sqrt(2/self.layer[i][0].data[\"IN\"])\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        z = x\n",
        "        for i in range(self.data[\"LayerNumber\"] - 1):\n",
        "            z = self.layer[i][1].forward((self.layer[i][0].forward(z)))\n",
        "\n",
        "        loss = self.layer[self.data[\"LayerNumber\"] - 1][1].forward(self.layer[self.data[\"LayerNumber\"] - 1][0].forward(z), t)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        delta = self.layer[self.data[\"LayerNumber\"] - 1][0].backward(self.layer[self.data[\"LayerNumber\"] - 1][1].backward())\n",
        "        for i in range(self.data[\"LayerNumber\"] - 2, -1, -1):\n",
        "            delta = self.layer[i][0].backward(self.layer[i][1].backward(delta))\n",
        "\n",
        "    def optimize(self):\n",
        "        for i in range(self.data[\"LayerNumber\"]):\n",
        "            self.optimizer[i].learning(self.layer[i][0].W, self.layer[i][0].dW, self.layer[i][0].b, self.layer[i][0].db)\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        z = x\n",
        "        for i in range(self.data[\"LayerNumber\"] - 1):\n",
        "            z = self.layer[i][1].test_forward((self.layer[i][0].test_forward(z)))\n",
        "\n",
        "        y = self.layer[self.data[\"LayerNumber\"] - 1][1].test_forward(self.layer[self.data[\"LayerNumber\"] - 1][0].test_forward(z))\n",
        "\n",
        "        a = np.argmax(y, axis=1)\n",
        "        b = np.argmax(t, axis=1)\n",
        "        accuracy = float(np.sum(a == b))/float(x.shape[0])\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
        "\n",
        "x_train  = x_train.reshape(60000, 784)\n",
        "x_test   = x_test.reshape(10000, 784)\n",
        "x_train  = x_train.astype('float32')\n",
        "x_test   = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test  /= 255\n",
        "t_train  = keras.utils.np_utils.to_categorical(t_train, 10)\n",
        "t_test   = keras.utils.np_utils.to_categorical(t_test, 10)\n",
        "\n",
        "batch_size = 100\n",
        "net1 = NeuralNetwork(inputSize=x_train.shape[1], outputSize=t_train.shape[1], layerNumber = 5, neuronNumber = 100, inputLayer = \"Swish\", optimizer = \"Adam\", learningRate = 0.001)\n",
        "net1.he()\n",
        "\n",
        "for epoch in range(10):\n",
        "    if epoch < 10:\n",
        "        print(\"{}エポック:認識精度{:.2f}%\".format(\"00\" + str(epoch), net1.accuracy(x_test, t_test)*100))\n",
        "    \n",
        "    elif epoch < 100:\n",
        "        print(\"{}エポック:認識精度{:.2f}%\".format(\"0\" + str(epoch), net1.accuracy(x_test, t_test)*100))\n",
        "\n",
        "    for i in range(int(x_train.shape[0]/batch_size)):\n",
        "        batch_mask = np.random.choice(x_train.shape[0], batch_size)\n",
        "        x_batch = x_train[batch_mask]\n",
        "        t_batch = t_train[batch_mask]\n",
        "\n",
        "        net1.forward(x_batch, t_batch)\n",
        "        net1.backward()\n",
        "        net1.optimize()\n",
        "\n",
        "print(\"{}エポック:認識精度{:.2f}%\".format(\"0\" + str(epoch + 1), net1.accuracy(x_test, t_test)*100))\n",
        "print(\"---------------------------------\")\n",
        "\"\"\"\n",
        "\n",
        "net = NeuralNetwork()\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ここにprint関数で表示させたい情報(ニューラルネットワークの実態の情報)を表示させるようにする。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlb6E4iQPYxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16056032-3921-49a3-a2a6-edc1539b334d"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([[-1,1,1,1,1],\n",
        "              [1,-1,1,1,1],\n",
        "              [1,1,-1,1,1],\n",
        "              [1,1,1,-1,1],\n",
        "              [1,1,1,1,-1],])\n",
        "w = np.array([[1,2,3],\n",
        "              [2,3,4],\n",
        "              [3,4,5],\n",
        "              [3,2,1],\n",
        "              [1,1,1]])\n",
        "print(1/np.sqrt(w*w))\n",
        "w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.5        0.33333333]\n",
            " [0.5        0.33333333 0.25      ]\n",
            " [0.33333333 0.25       0.2       ]\n",
            " [0.33333333 0.5        1.        ]\n",
            " [1.         1.         1.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [2, 3, 4],\n",
              "       [3, 4, 5],\n",
              "       [3, 2, 1],\n",
              "       [1, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5uq-JH6GLKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce62d42-9df2-4794-a6a3-0ad8566dc2e0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class C:\n",
        "    def __init__(self):\n",
        "        self.x = np.random.randn(5)\n",
        "\n",
        "class K:\n",
        "    def __init__(self):\n",
        "        self.v = None\n",
        "    \n",
        "    def eee(self, x, lr = 0.01, momentum = 10):\n",
        "        if self.v is None:\n",
        "            self.v = np.zeros_like(x)\n",
        "        \n",
        "        self.v = momentum*self.v - lr*x\n",
        "        x += self.v \n",
        "\n",
        "class G:\n",
        "    def __init__(self):\n",
        "        self.layer = C()\n",
        "        self.op = K()\n",
        "\n",
        "    def momentum(self):\n",
        "        self.op.eee(self.layer.x)\n",
        "\n",
        "a = G()\n",
        "print(a.layer.x)\n",
        "a.momentum()\n",
        "print(a.layer.x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.68319804 -0.04941163 -0.54388934 -3.44836962 -0.25866701]\n",
            "[-0.67636606 -0.04891752 -0.53845045 -3.41388592 -0.25608034]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q0bN5N42nVQ",
        "outputId": "e934f15a-2d9c-4fe0-d522-ebfefe01ff79"
      },
      "source": [
        "print(type(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'int'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqWnk6hQ2vPr",
        "outputId": "eb117e92-8852-470d-b5d2-d0f5e878f3d8"
      },
      "source": [
        "import numpy as np\n",
        "a = list(np.random.randint(-5, 5, (10)))\n",
        "print(a)\n",
        "print((False in list(np.array(a) > 0)))\n",
        "print(True in list(np.array(a) == int))\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, -1, -5, 4, 2, -2, 2, -4, 1, 2]\n",
            "True\n",
            "False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, -1, -5, 4, 2, -2, 2, -4, 1, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7kZBINsYUTb",
        "outputId": "b231e29d-4dde-4bec-df5e-0cfcf41022df"
      },
      "source": [
        "[1]*5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnCVkUIwcq4x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}