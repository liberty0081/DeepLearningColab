{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworkLayer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN00dnSCPj3cRM2d6phrYAr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liberty0081/DeepLearningColab/blob/develop/NeuralNetworkLayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UEbvDDs-VJM"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Softmax2CrossEntropy():\n",
        "    def __init__(self):\n",
        "        self.y    = None\n",
        "        self.t    = None\n",
        "        self.flag = False\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.y = np.empty(x.shape)\n",
        "        self.t = t\n",
        "        \n",
        "        for i in range(self.y.shape[0]):\n",
        "            c = np.amax(x[i])\n",
        "            self.y[i] = np.exp(x[i] - c)/(np.sum(np.exp(x[i] - c)))\n",
        "\n",
        "        self.flag = True\n",
        "        \n",
        "        return -(np.sum(self.t*np.log(self.y)))/self.t.shape[0]\n",
        "\n",
        "    def test_forward(self, x):\n",
        "        y = np.empty(x.shape)\n",
        "\n",
        "        for i in range(y.shape[0]):\n",
        "            c = np.amax(x[i])\n",
        "            y[i] = np.exp(x[i] - c)/(np.sum(np.exp(x[i] - c)))\n",
        "        \n",
        "        return y\n",
        "\n",
        "    def test_loss(self, x, t):\n",
        "        y = self.test_forward(x)\n",
        "        return -(np.sum(t*np.log(y)))/t.shape[0]\n",
        "\n",
        "    def backward(self):\n",
        "        if self.flag:\n",
        "            self.flag = False\n",
        "            return (self.y - self.t)/self.t.shape[0]\n",
        "        \n",
        "        else:\n",
        "            raise Exception(\"you have to run forward method before\")\n",
        "            \n",
        "class Swish():\n",
        "    def __init__(self):\n",
        "        self.x    = None\n",
        "        self.flag = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x    = x\n",
        "        self.flag = True\n",
        "        return self.x/(1 + np.exp(-self.x))\n",
        "\n",
        "    def test_forward(self, x):\n",
        "        return x/(1 + np.exp(-x))\n",
        "\n",
        "    def backward(self, delta):\n",
        "        if self.flag:\n",
        "            self.flag = False\n",
        "            return delta*((1 + np.exp(-self.x)*(1 + self.x))/((1 + np.exp(-self.x))**2))\n",
        "        \n",
        "        else:\n",
        "            raise Exception(\"you have to run forward method before\")\n",
        "\n",
        "class ReLU():\n",
        "    def __init__(self):\n",
        "        self.x    = None\n",
        "        self.flag = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x    = x\n",
        "        self.flag = True\n",
        "        return np.maximum(0, self.x)\n",
        "\n",
        "    def test_forward(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def backward(self, delta):\n",
        "        if self.flag:\n",
        "            self.flag = False\n",
        "            return delta*((self.x > 0).astype(np.int))\n",
        "        \n",
        "        else:\n",
        "            raise Exception(\"you have to run forward method before\")\n",
        "\n",
        "class Sigmoid():\n",
        "    def __init__(self):\n",
        "        self.x    = None\n",
        "        self.flag = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x    = x\n",
        "        self.flag = True\n",
        "        return 1/(1 + np.exp(-self.x))\n",
        "\n",
        "    def test_forward(self, x):\n",
        "        return 1/(1 + np.exp(-x))\n",
        "\n",
        "    def backward(self, delta):\n",
        "        if self.flag:\n",
        "            self.flag = False\n",
        "            return delta*((np.exp(-self.x))/(1 + np.exp(-self.x))**2)\n",
        "        \n",
        "        else:\n",
        "            raise Exception(\"you have to run forward method before\")\n",
        "\n",
        "class Affine():\n",
        "    def __init__(self, inputSize, outputSize):\n",
        "        self.data  = {\"IN\":inputSize, \"OUT\":outputSize}\n",
        "        self.W     = np.random.randn(self.data[\"IN\"], self.data[\"OUT\"])*0.1\n",
        "        self.b     = np.random.randn(self.data[\"OUT\"])*0.1\n",
        "        self.dW    = np.zeros_like(self.W)\n",
        "        self.db    = np.zeros_like(self.b)\n",
        "        self.delta = None\n",
        "        self.x     = None\n",
        "        self.flag  = False \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x    = x\n",
        "        self.flag = True\n",
        "\n",
        "        return np.dot(self.x, self.W) + self.b\n",
        "\n",
        "    def test_forward(self, x):\n",
        "        return np.dot(x, self.W) + self.b\n",
        "\n",
        "    def backward(self, delta):\n",
        "        if self.flag:\n",
        "            self.flag = False\n",
        "            self.delta = delta\n",
        "            self.dW = np.dot(self.x.T, self.delta)\n",
        "            self.db = np.sum(self.delta, axis = 0)\n",
        "\n",
        "            return np.dot(self.delta, self.W.T)\n",
        "        \n",
        "        else:\n",
        "            raise Exception(\"you have to run forward method before\")\n",
        "\n",
        "class BatchNorm():\n",
        "    def __init__(self):\n",
        "        self.W      = None\n",
        "        self.b      = None\n",
        "        self.mu     = None\n",
        "        self.sigma  = None\n",
        "        self.m      = None\n",
        "        self.dW     = None\n",
        "        self.db     = None\n",
        "        self.x      = None\n",
        "        self.x_hat  = None\n",
        "        self.delta  = None\n",
        "        self.flag   = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x    = x\n",
        "        self.m    = self.x.shape[0]\n",
        "        self.flag = True\n",
        "\n",
        "        if self.W is None:\n",
        "            self.W = np.ones_like(self.x)\n",
        "        \n",
        "        if self.b is None:\n",
        "            self.b = np.zeros_like(self.x)\n",
        "\n",
        "        self.mu    = np.sum(self.x, axis = 0)/self.x.shape[0]\n",
        "        self.sigma = np.sum((self.x - self.mu)**2, axis = 0)/self.x.shape[0]\n",
        "        self.x_hat = ((self.x - self.mu)/np.sqrt(self.sigma + 1e-7))\n",
        "\n",
        "        return self.W*self.x_hat + self.b\n",
        "\n",
        "    def test_forward(self, x):\n",
        "        m     = x.shape[0]\n",
        "        mu    = np.sum(x, axis = 0)/x.shape[0]\n",
        "        sigma = np.sum((x - mu)**2, axis = 0)/x.shape[0]\n",
        "        x_hat = ((x - mu)/np.sqrt(sigma + 1e-7))\n",
        "\n",
        "        if self.W is None or self.b is None:\n",
        "            return 1*x_hat\n",
        "\n",
        "        else:\n",
        "            return self.W*x_hat + self.b\n",
        "\n",
        "\n",
        "    def backward(self, delta):\n",
        "        if self.flag:\n",
        "            self.flag = False\n",
        "\n",
        "            self.db   = delta\n",
        "            self.dW   = delta*self.x_hat\n",
        "        \n",
        "            a         = ((self.x - self.mu) - np.sum(self.x - self.mu, axis = 0)/self.m)*2/self.m\n",
        "\n",
        "            row_size1    = a.shape[0]\n",
        "            column_size1 = a.shape[1]\n",
        "\n",
        "            b = a.T.flatten()\n",
        "            dsigma = np.zeros(column_size1*row_size1*column_size1)\n",
        "\n",
        "            indx1  = (np.arange(a.size)*column_size1 + np.floor(np.arange(a.size)/row_size1)).astype(int)\n",
        "            dsigma[indx1] = b\n",
        "            dsigma = dsigma.reshape(column_size1, row_size1, column_size1)\n",
        "\n",
        "            dx_hat =  ((np.resize(np.identity(self.m), (self.sigma.size, self.m, self.m)) - 1/self.m)*((self.W*(self.sigma + 1e-7)).T)[:, np.newaxis, np.newaxis]) \\\n",
        "            - np.dot(dsigma, (self.W*(self.x - self.mu)/2).T)/(np.sqrt((self.sigma + 1e-7)[:, np.newaxis, np.newaxis])*(self.sigma + 1e-7)[:, np.newaxis, np.newaxis])\n",
        "\n",
        "            c = np.dot(dx_hat, delta)\n",
        "            row_size2    = c.shape[1]\n",
        "            column_size2 = c.shape[2]\n",
        "            indx2 = (np.arange(row_size2*column_size2)*column_size2 + np.floor(np.arange(row_size2*column_size2)/row_size2)).astype(int)\n",
        "\n",
        "            return c.flatten()[indx2].reshape(column_size2, row_size2)\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"you have to run forward method before\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}